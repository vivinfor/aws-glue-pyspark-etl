{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 AWS Glue & PySpark ETL: Análise de Transações Financeiras\n",
    "\n",
    "# 1️⃣ Introdução e Objetivo\n",
    "\n",
    "Este notebook implementa a etapa de **ETL (Extração, Transformação e Carga)** para processamento dos dados brutos.\n",
    "\n",
    "📌 **Objetivo:**  \n",
    "- Carregar os dados brutos da pasta `data/raw/`\n",
    "- Aplicar transformações com **PySpark** (limpeza, normalização e regras de negócio)\n",
    "- Armazenar os dados processados em formato **Parquet** para análise posterior\n",
    "\n",
    "🔧 **Tecnologias utilizadas:**  \n",
    "- **PySpark** para processamento eficiente de grandes volumes de dados\n",
    "- **AWS Glue** (se rodarmos na nuvem) ou Spark local\n",
    "- **Parquet** como formato de armazenamento otimizado\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Configuração do do PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criar uma sessão do PySpark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Processing\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ PySpark configurado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Carregamento das Configurações (config.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# 📌 **Determinar caminho absoluto para `config.yaml` na raiz do projeto**\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Sobe um nível para a raiz do projeto\n",
    "config_path = os.path.join(base_path, \"config\", \"config.yaml\")\n",
    "\n",
    "print(f\"📂 Tentando carregar: {config_path}\")\n",
    "\n",
    "# 📌 **Verificar se o arquivo `config.yaml` existe**\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"✅ Configuração carregada com sucesso!\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"❌ Arquivo 'config.yaml' não encontrado! Verifique o caminho: {config_path}\")\n",
    "\n",
    "# 📌 **Ajustar caminhos dos dados**\n",
    "raw_data_path = os.path.normpath(config.get(\"raw_data_path\", \"data/raw/\"))\n",
    "processed_data_path = os.path.normpath(config.get(\"data_path\", \"data/processed/\"))\n",
    "\n",
    "# 📌 **Garantir que os caminhos sejam absolutos**\n",
    "if not os.path.isabs(raw_data_path):\n",
    "    raw_data_path = os.path.abspath(os.path.join(base_path, raw_data_path))\n",
    "if not os.path.isabs(processed_data_path):\n",
    "    processed_data_path = os.path.abspath(os.path.join(base_path, processed_data_path))\n",
    "\n",
    "# 📌 **Verificar se os diretórios existem**\n",
    "if not os.path.exists(raw_data_path):\n",
    "    raise FileNotFoundError(f\"❌ ERRO: O diretório de dados brutos '{raw_data_path}' não existe!\")\n",
    "if not os.path.exists(processed_data_path):\n",
    "    os.makedirs(processed_data_path)  # Criar o diretório se não existir\n",
    "\n",
    "# 📌 **Exibir as configurações carregadas**\n",
    "print(f\"📂 Caminho dos dados brutos: {raw_data_path}\")\n",
    "print(f\"📂 Caminho dos dados processados: {processed_data_path}\")\n",
    "\n",
    "print(\"✅ Configurações carregadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Detectar e Carregar os Dados Brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, when, date_format, unix_timestamp, lag, concat, lit, count\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "\n",
    "# 📌 **Criar sessão Spark (caso ainda não tenha sido criada)**\n",
    "spark = SparkSession.builder.appName(\"ETL - Fraude Financeira\").getOrCreate()\n",
    "\n",
    "# 📌 **Definir caminhos a partir da configuração**\n",
    "raw_file_path = os.path.join(raw_data_path, \"dados-brutos.csv\")\n",
    "\n",
    "# 📌 **Definir caminho de saída**\n",
    "output_path = os.path.normpath(config.get(\"processed_data_path\", \"data/processed/\"))\n",
    "if not os.path.isabs(output_path):\n",
    "    output_path = os.path.abspath(output_path)\n",
    "\n",
    "# 📌 **Testar separadores comuns**\n",
    "separators = [\",\", \"|\", \";\", \"\\t\"]\n",
    "for sep in separators:\n",
    "    try:\n",
    "        df = spark.read.csv(raw_file_path, header=True, inferSchema=True, sep=sep)\n",
    "        if len(df.columns) > 5:  # Se há mais de 5 colunas, provavelmente o separador está correto\n",
    "            print(f\"✅ Separador correto detectado: '{sep}'\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro com separador '{sep}': {str(e)}\")\n",
    "\n",
    "# 📌 **Verificar se o DataFrame foi carregado corretamente**\n",
    "if \"df\" not in locals():\n",
    "    raise ValueError(\"❌ ERRO: Falha ao carregar os dados! Verifique o formato do arquivo.\")\n",
    "\n",
    "# 📌 **Verificar colunas antes do processamento**\n",
    "print(f\"📋 Colunas detectadas antes do processamento: {df.columns}\")\n",
    "\n",
    "# 📌 **Remover duplicatas**\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# 📌 **Remover registros onde colunas críticas sejam nulas**\n",
    "df = df.na.drop(subset=[\"cc_num\", \"amt\", \"is_fraud\"])\n",
    "\n",
    "# 📌 **Preencher valores nulos em colunas opcionais**\n",
    "df = df.fillna({\n",
    "    \"merchant\": \"Desconhecido\",\n",
    "    \"city\": \"Não informado\",\n",
    "    \"state\": \"Não informado\",\n",
    "    \"lat\": 0.0,\n",
    "    \"long\": 0.0\n",
    "})\n",
    "\n",
    "# 📌 **Criar coluna combinando data e hora**\n",
    "df = df.withColumn(\"trans_date_trans_time\", concat(col(\"trans_date\"), lit(\" \"), col(\"trans_time\")))\n",
    "df = df.withColumn(\"trans_date_trans_time\", col(\"trans_date_trans_time\").cast(\"timestamp\"))\n",
    "\n",
    "# 📌 **Criar colunas de dia da semana e horário**\n",
    "df = df.withColumn(\"day_of_week\", date_format(col(\"trans_date_trans_time\"), \"E\"))\n",
    "df = df.withColumn(\"hour_of_day\", date_format(col(\"trans_date_trans_time\"), \"HH\").cast(\"int\"))\n",
    "\n",
    "# 📌 **Criar coluna categorizando período da transação**\n",
    "df = df.withColumn(\n",
    "    \"transaction_period\",\n",
    "    when(col(\"hour_of_day\") < 6, \"Madrugada\")\n",
    "    .when(col(\"hour_of_day\") < 12, \"Manhã\")\n",
    "    .when(col(\"hour_of_day\") < 18, \"Tarde\")\n",
    "    .otherwise(\"Noite\")\n",
    ")\n",
    "\n",
    "# 📌 **Criar flag para transações acima de 10.000**\n",
    "df = df.withColumn(\"possible_fraud_high_value\", (col(\"amt\") > 10000).cast(\"integer\"))\n",
    "\n",
    "# 📌 **Definir janela para detecção de transações rápidas**\n",
    "window_spec_time = Window.partitionBy(\"cc_num\", \"merchant\").orderBy(\"trans_date_trans_time\")\n",
    "df = df.withColumn(\"time_diff\", unix_timestamp(\"trans_date_trans_time\") - lag(unix_timestamp(\"trans_date_trans_time\")).over(window_spec_time))\n",
    "df = df.withColumn(\"possible_fraud_fast_transactions\", (col(\"time_diff\") < 10).cast(\"integer\"))\n",
    "\n",
    "# 📌 **Ajuste de tipos**\n",
    "df = df.withColumn(\"cc_num\", col(\"cc_num\").cast(\"string\"))\n",
    "df = df.withColumn(\"amt\", col(\"amt\").cast(\"float\"))\n",
    "df = df.withColumn(\"zip\", col(\"zip\").cast(\"int\"))\n",
    "df = df.withColumn(\"lat\", col(\"lat\").cast(\"float\"))\n",
    "df = df.withColumn(\"long\", col(\"long\").cast(\"float\"))\n",
    "df = df.withColumn(\"city_pop\", col(\"city_pop\").cast(\"int\"))\n",
    "df = df.withColumn(\"dob\", col(\"dob\").cast(\"string\"))\n",
    "df = df.withColumn(\"unix_time\", col(\"unix_time\").cast(\"int\"))\n",
    "df = df.withColumn(\"merch_lat\", col(\"merch_lat\").cast(\"float\"))\n",
    "df = df.withColumn(\"merch_long\", col(\"merch_long\").cast(\"float\"))\n",
    "df = df.withColumn(\"is_fraud\", col(\"is_fraud\").cast(\"int\"))\n",
    "df = df.withColumn(\"possible_fraud_high_value\", col(\"possible_fraud_high_value\").cast(\"int\"))\n",
    "df = df.withColumn(\"possible_fraud_fast_transactions\", col(\"possible_fraud_fast_transactions\").cast(\"int\"))\n",
    "\n",
    "# 📌 **Contagem de valores nulos**\n",
    "null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "null_counts.show()\n",
    "\n",
    "# 📌 **Criar diretório de saída**\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# 📌 **Contagem final de registros**\n",
    "print(f\"Total de registros processados: {df.count()}\")\n",
    "\n",
    "# 📌 **Salvar os dados processados em Parquet**\n",
    "df.write.mode(\"overwrite\").partitionBy(\"category\").parquet(output_path)\n",
    "\n",
    "print(\"✅ Dados brutos carregados com Sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Aplicar Schema e Ajustes Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# 📌 **Definir Schema**\n",
    "schema = StructType([\n",
    "    StructField(\"trans_date_trans_time\", TimestampType(), False),\n",
    "    StructField(\"cc_num\", StringType(), False),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amt\", DoubleType(), False),\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", IntegerType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"long\", DoubleType(), True),\n",
    "    StructField(\"city_pop\", IntegerType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True),\n",
    "    StructField(\"trans_num\", StringType(), False),\n",
    "    StructField(\"unix_time\", IntegerType(), False),\n",
    "    StructField(\"merch_lat\", DoubleType(), True),\n",
    "    StructField(\"merch_long\", DoubleType(), True),\n",
    "    StructField(\"is_fraud\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# 📌 **Carregar os dados processados e aplicar schema**\n",
    "df = spark.read.schema(schema).parquet(output_path)\n",
    "print(f\"✅ Dados carregados com {df.count()} registros após aplicação de schema.\")\n",
    "\n",
    "# 📌 **Mostrar schema final**\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Salvar Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "partition_path = f\"{processed_data_path}/category=grocery_net\"\n",
    "if os.path.exists(partition_path):\n",
    "    shutil.rmtree(partition_path)\n",
    "    print(f\"🗑️ Partição {partition_path} removida para evitar conflito de schema.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
