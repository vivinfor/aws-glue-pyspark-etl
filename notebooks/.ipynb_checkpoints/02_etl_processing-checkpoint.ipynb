{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š AWS Glue & PySpark ETL: AnÃ¡lise de TransaÃ§Ãµes Financeiras\n",
    "\n",
    "# 1ï¸âƒ£ IntroduÃ§Ã£o e Objetivo\n",
    "\n",
    "Este notebook implementa a etapa de **ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga)** para processamento dos dados brutos.\n",
    "\n",
    "ğŸ“Œ **Objetivo:**  \n",
    "- Carregar os dados brutos da pasta `data/raw/`\n",
    "- Aplicar transformaÃ§Ãµes com **PySpark** (limpeza, normalizaÃ§Ã£o e regras de negÃ³cio)\n",
    "- Armazenar os dados processados em formato **Parquet** para anÃ¡lise posterior\n",
    "\n",
    "ğŸ”§ **Tecnologias utilizadas:**  \n",
    "- **PySpark** para processamento eficiente de grandes volumes de dados\n",
    "- **AWS Glue** (se rodarmos na nuvem) ou Spark local\n",
    "- **Parquet** como formato de armazenamento otimizado\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ConfiguraÃ§Ã£o do do PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criar uma sessÃ£o do PySpark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Processing\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… PySpark configurado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Carregamento das ConfiguraÃ§Ãµes (config.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# ğŸ“Œ **Determinar caminho absoluto para `config.yaml` na raiz do projeto**\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Sobe um nÃ­vel para a raiz do projeto\n",
    "config_path = os.path.join(base_path, \"config\", \"config.yaml\")\n",
    "\n",
    "print(f\"ğŸ“‚ Tentando carregar: {config_path}\")\n",
    "\n",
    "# ğŸ“Œ **Verificar se o arquivo `config.yaml` existe**\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"âœ… ConfiguraÃ§Ã£o carregada com sucesso!\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"âŒ Arquivo 'config.yaml' nÃ£o encontrado! Verifique o caminho: {config_path}\")\n",
    "\n",
    "# ğŸ“Œ **Ajustar caminhos dos dados**\n",
    "raw_data_path = os.path.normpath(config.get(\"raw_data_path\", \"data/raw/\"))\n",
    "processed_data_path = os.path.normpath(config.get(\"data_path\", \"data/processed/\"))\n",
    "\n",
    "# ğŸ“Œ **Garantir que os caminhos sejam absolutos**\n",
    "if not os.path.isabs(raw_data_path):\n",
    "    raw_data_path = os.path.abspath(os.path.join(base_path, raw_data_path))\n",
    "if not os.path.isabs(processed_data_path):\n",
    "    processed_data_path = os.path.abspath(os.path.join(base_path, processed_data_path))\n",
    "\n",
    "# ğŸ“Œ **Verificar se os diretÃ³rios existem**\n",
    "if not os.path.exists(raw_data_path):\n",
    "    raise FileNotFoundError(f\"âŒ ERRO: O diretÃ³rio de dados brutos '{raw_data_path}' nÃ£o existe!\")\n",
    "if not os.path.exists(processed_data_path):\n",
    "    os.makedirs(processed_data_path)  # Criar o diretÃ³rio se nÃ£o existir\n",
    "\n",
    "# ğŸ“Œ **Exibir as configuraÃ§Ãµes carregadas**\n",
    "print(f\"ğŸ“‚ Caminho dos dados brutos: {raw_data_path}\")\n",
    "print(f\"ğŸ“‚ Caminho dos dados processados: {processed_data_path}\")\n",
    "\n",
    "print(\"âœ… ConfiguraÃ§Ãµes carregadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Detectar e Carregar os Dados Brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, when, date_format, unix_timestamp, lag, concat, lit, count\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "\n",
    "# ğŸ“Œ **Criar sessÃ£o Spark (caso ainda nÃ£o tenha sido criada)**\n",
    "spark = SparkSession.builder.appName(\"ETL - Fraude Financeira\").getOrCreate()\n",
    "\n",
    "# ğŸ“Œ **Definir caminhos a partir da configuraÃ§Ã£o**\n",
    "raw_file_path = os.path.join(raw_data_path, \"dados-brutos.csv\")\n",
    "\n",
    "# ğŸ“Œ **Definir caminho de saÃ­da**\n",
    "output_path = os.path.normpath(config.get(\"processed_data_path\", \"data/processed/\"))\n",
    "if not os.path.isabs(output_path):\n",
    "    output_path = os.path.abspath(output_path)\n",
    "\n",
    "# ğŸ“Œ **Testar separadores comuns**\n",
    "separators = [\",\", \"|\", \";\", \"\\t\"]\n",
    "for sep in separators:\n",
    "    try:\n",
    "        df = spark.read.csv(raw_file_path, header=True, inferSchema=True, sep=sep)\n",
    "        if len(df.columns) > 5:  # Se hÃ¡ mais de 5 colunas, provavelmente o separador estÃ¡ correto\n",
    "            print(f\"âœ… Separador correto detectado: '{sep}'\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erro com separador '{sep}': {str(e)}\")\n",
    "\n",
    "# ğŸ“Œ **Verificar se o DataFrame foi carregado corretamente**\n",
    "if \"df\" not in locals():\n",
    "    raise ValueError(\"âŒ ERRO: Falha ao carregar os dados! Verifique o formato do arquivo.\")\n",
    "\n",
    "# ğŸ“Œ **Verificar colunas antes do processamento**\n",
    "print(f\"ğŸ“‹ Colunas detectadas antes do processamento: {df.columns}\")\n",
    "\n",
    "# ğŸ“Œ **Remover duplicatas**\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# ğŸ“Œ **Remover registros onde colunas crÃ­ticas sejam nulas**\n",
    "df = df.na.drop(subset=[\"cc_num\", \"amt\", \"is_fraud\"])\n",
    "\n",
    "# ğŸ“Œ **Preencher valores nulos em colunas opcionais**\n",
    "df = df.fillna({\n",
    "    \"merchant\": \"Desconhecido\",\n",
    "    \"city\": \"NÃ£o informado\",\n",
    "    \"state\": \"NÃ£o informado\",\n",
    "    \"lat\": 0.0,\n",
    "    \"long\": 0.0\n",
    "})\n",
    "\n",
    "# ğŸ“Œ **Criar coluna combinando data e hora**\n",
    "df = df.withColumn(\"trans_date_trans_time\", concat(col(\"trans_date\"), lit(\" \"), col(\"trans_time\")))\n",
    "df = df.withColumn(\"trans_date_trans_time\", col(\"trans_date_trans_time\").cast(\"timestamp\"))\n",
    "\n",
    "# ğŸ“Œ **Criar colunas de dia da semana e horÃ¡rio**\n",
    "df = df.withColumn(\"day_of_week\", date_format(col(\"trans_date_trans_time\"), \"E\"))\n",
    "df = df.withColumn(\"hour_of_day\", date_format(col(\"trans_date_trans_time\"), \"HH\").cast(\"int\"))\n",
    "\n",
    "# ğŸ“Œ **Criar coluna categorizando perÃ­odo da transaÃ§Ã£o**\n",
    "df = df.withColumn(\n",
    "    \"transaction_period\",\n",
    "    when(col(\"hour_of_day\") < 6, \"Madrugada\")\n",
    "    .when(col(\"hour_of_day\") < 12, \"ManhÃ£\")\n",
    "    .when(col(\"hour_of_day\") < 18, \"Tarde\")\n",
    "    .otherwise(\"Noite\")\n",
    ")\n",
    "\n",
    "# ğŸ“Œ **Criar flag para transaÃ§Ãµes acima de 10.000**\n",
    "df = df.withColumn(\"possible_fraud_high_value\", (col(\"amt\") > 10000).cast(\"integer\"))\n",
    "\n",
    "# ğŸ“Œ **Definir janela para detecÃ§Ã£o de transaÃ§Ãµes rÃ¡pidas**\n",
    "window_spec_time = Window.partitionBy(\"cc_num\", \"merchant\").orderBy(\"trans_date_trans_time\")\n",
    "df = df.withColumn(\"time_diff\", unix_timestamp(\"trans_date_trans_time\") - lag(unix_timestamp(\"trans_date_trans_time\")).over(window_spec_time))\n",
    "df = df.withColumn(\"possible_fraud_fast_transactions\", (col(\"time_diff\") < 10).cast(\"integer\"))\n",
    "\n",
    "# ğŸ“Œ **Ajuste de tipos**\n",
    "df = df.withColumn(\"cc_num\", col(\"cc_num\").cast(\"string\"))\n",
    "df = df.withColumn(\"amt\", col(\"amt\").cast(\"float\"))\n",
    "df = df.withColumn(\"zip\", col(\"zip\").cast(\"int\"))\n",
    "df = df.withColumn(\"lat\", col(\"lat\").cast(\"float\"))\n",
    "df = df.withColumn(\"long\", col(\"long\").cast(\"float\"))\n",
    "df = df.withColumn(\"city_pop\", col(\"city_pop\").cast(\"int\"))\n",
    "df = df.withColumn(\"dob\", col(\"dob\").cast(\"string\"))\n",
    "df = df.withColumn(\"unix_time\", col(\"unix_time\").cast(\"int\"))\n",
    "df = df.withColumn(\"merch_lat\", col(\"merch_lat\").cast(\"float\"))\n",
    "df = df.withColumn(\"merch_long\", col(\"merch_long\").cast(\"float\"))\n",
    "df = df.withColumn(\"is_fraud\", col(\"is_fraud\").cast(\"int\"))\n",
    "df = df.withColumn(\"possible_fraud_high_value\", col(\"possible_fraud_high_value\").cast(\"int\"))\n",
    "df = df.withColumn(\"possible_fraud_fast_transactions\", col(\"possible_fraud_fast_transactions\").cast(\"int\"))\n",
    "\n",
    "# ğŸ“Œ **Contagem de valores nulos**\n",
    "null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "null_counts.show()\n",
    "\n",
    "# ğŸ“Œ **Criar diretÃ³rio de saÃ­da**\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# ğŸ“Œ **Contagem final de registros**\n",
    "print(f\"Total de registros processados: {df.count()}\")\n",
    "\n",
    "# ğŸ“Œ **Salvar os dados processados em Parquet**\n",
    "df.write.mode(\"overwrite\").partitionBy(\"category\").parquet(output_path)\n",
    "\n",
    "print(\"âœ… Dados brutos carregados com Sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Aplicar Schema e Ajustes Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# ğŸ“Œ **Definir Schema**\n",
    "schema = StructType([\n",
    "    StructField(\"trans_date_trans_time\", TimestampType(), False),\n",
    "    StructField(\"cc_num\", StringType(), False),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amt\", DoubleType(), False),\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", IntegerType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"long\", DoubleType(), True),\n",
    "    StructField(\"city_pop\", IntegerType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True),\n",
    "    StructField(\"trans_num\", StringType(), False),\n",
    "    StructField(\"unix_time\", IntegerType(), False),\n",
    "    StructField(\"merch_lat\", DoubleType(), True),\n",
    "    StructField(\"merch_long\", DoubleType(), True),\n",
    "    StructField(\"is_fraud\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# ğŸ“Œ **Carregar os dados processados e aplicar schema**\n",
    "df = spark.read.schema(schema).parquet(output_path)\n",
    "print(f\"âœ… Dados carregados com {df.count()} registros apÃ³s aplicaÃ§Ã£o de schema.\")\n",
    "\n",
    "# ğŸ“Œ **Mostrar schema final**\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Salvar Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "partition_path = f\"{processed_data_path}/category=grocery_net\"\n",
    "if os.path.exists(partition_path):\n",
    "    shutil.rmtree(partition_path)\n",
    "    print(f\"ğŸ—‘ï¸ PartiÃ§Ã£o {partition_path} removida para evitar conflito de schema.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
