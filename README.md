# AWS Glue & PySpark ETL

## ğŸ“Œ Objetivo
Este projeto implementa um **pipeline de ETL escalÃ¡vel** usando **AWS Glue e PySpark** para processamento, transformaÃ§Ã£o e validaÃ§Ã£o de dados transacionais. O pipeline garante qualidade, consistÃªncia e eficiÃªncia no tratamento de grandes volumes de dados, aplicando tÃ©cnicas avanÃ§adas para **limpeza, remoÃ§Ã£o de outliers, validaÃ§Ã£o de schema e enriquecimento dos dados**. AlÃ©m disso, agora exploramos **otimizaÃ§Ã£o de consultas SQL**, **anÃ¡lises avanÃ§adas com Python** e **visualizaÃ§Ã£o interativa de dados com Power BI**.

## ğŸ”¹ Fonte dos Dados
Os dados utilizados neste projeto sÃ£o provenientes de um dataset pÃºblico disponibilizado pelo Kaggle, referente a **transaÃ§Ãµes financeiras sintÃ©ticas**. O dataset foi gerado atravÃ©s da ferramenta **Sparkov Data Generation**, simulando transaÃ§Ãµes de janeiro a dezembro de 2023.

### ğŸ“Œ Link para o dataset:
[Kaggle - Fraude em TransaÃ§Ãµes de CartÃ£o de CrÃ©dito](https://www.kaggle.com/competitions/fraude-em-transaes-de-carto-de-crdito/data)

O dataset contÃ©m informaÃ§Ãµes como:
- **Dados da transaÃ§Ã£o** (data, valor, comerciante, categoria, localizaÃ§Ã£o, etc.).
- **InformaÃ§Ãµes do titular do cartÃ£o** (nome, endereÃ§o, gÃªnero, profissÃ£o, etc.).
- **Indicador de fraude (`is_fraud`)** para identificar transaÃ§Ãµes fraudulentas.

## ğŸ”¹ Tecnologias Utilizadas
- **AWS Glue**: Para processamento e transformaÃ§Ã£o de dados em grande escala.
- **PySpark**: Para manipulaÃ§Ã£o e transformaÃ§Ã£o eficiente dos dados.
- **AWS S3**: Para armazenamento dos dados brutos e processados.
- **SQL**: Para anÃ¡lise e validaÃ§Ã£o de qualidade dos dados.
- **Jupyter Notebooks**: Para exploraÃ§Ã£o e visualizaÃ§Ã£o dos dados.
- **Power BI**: Para construÃ§Ã£o de dashboards interativos.

## ğŸš€ Pipeline de ETL e AnÃ¡lises
1. **ExtraÃ§Ã£o**: Carregamento do dataset pÃºblico armazenado no **S3**.
2. **TransformaÃ§Ã£o**:
   - RemoÃ§Ã£o de **valores nulos e duplicados** com base em regras de negÃ³cio.
   - **ValidaÃ§Ã£o da estrutura do dataset** de acordo com um schema predefinido (`config/schema.json`).
   - **DetecÃ§Ã£o e tratamento de outliers** usando **Z-score**.
   - **CriaÃ§Ã£o de novas colunas** para facilitar anÃ¡lises (dia da semana, horÃ¡rio da transaÃ§Ã£o, etc.).
   - CÃ¡lculo de **estatÃ­sticas descritivas** (mÃ©dia, mediana, desvio padrÃ£o, distribuiÃ§Ã£o de valores faltantes).
   - NormalizaÃ§Ã£o de colunas para padronizaÃ§Ã£o dos dados.
3. **OtimizaÃ§Ã£o de Consultas SQL**:
   - ComparaÃ§Ã£o entre **SQL puro, PySpark e Delta Lake**.
   - Testes de performance e eficiÃªncia nas consultas.
4. **Carga e VisualizaÃ§Ã£o**:
   - Salvamento do dataset transformado no S3 e banco de dados.
   - IntegraÃ§Ã£o com **Power BI** para construÃ§Ã£o de dashboards interativos.

## ğŸ”¹ Regras de NegÃ³cio Aplicadas
Durante o pipeline ETL, foram aplicadas regras para garantir a **qualidade dos dados e detecÃ§Ã£o de possÃ­veis fraudes**:

### **1ï¸âƒ£ Tratamento de Valores Nulos**
| Coluna | Regra |
|--------|-------|
| `cc_num` (CartÃ£o) | ğŸš¨ **ObrigatÃ³rio** â€“ Remover registros sem nÃºmero de cartÃ£o. |
| `amt` (Valor) | ğŸš¨ **ObrigatÃ³rio** â€“ TransaÃ§Ãµes sem valor devem ser descartadas. |
| `merchant` (Comerciante) | ğŸ”¹ **Opcional** â€“ Se vazio, preencher com "Desconhecido". |
| `city`, `state` | ğŸ”¹ **Opcional** â€“ Se vazios, preencher com "NÃ£o informado". |
| `lat`, `long` | ğŸ”¹ **Opcional** â€“ Se vazios, preencher com `0.0`. |
| `is_fraud` (Fraude) | ğŸš¨ **ObrigatÃ³rio** â€“ Se `null`, o registro deve ser descartado. |

### **2ï¸âƒ£ DetecÃ§Ã£o e RemoÃ§Ã£o de Outliers**
| Coluna | Regra |
|--------|-------|
| `amt` (Valor) | ğŸš¨ **Remover transaÃ§Ãµes fora de 3 desvios padrÃ£o (Z-score).** |
| `city_pop` (PopulaÃ§Ã£o) | ğŸš¨ **Remover registros com `city_pop < 100` (cidades nÃ£o realistas).** |

### **3ï¸âƒ£ Regras de DetecÃ§Ã£o de Fraudes**
| CritÃ©rio | Regra |
|----------|-------|
| **TransaÃ§Ãµes acima de $10,000** | ğŸš¨ **Alerta de possÃ­vel fraude** |
| **Mesma pessoa comprando em estados diferentes no mesmo dia** | ğŸš¨ **Alerta de possÃ­vel fraude** |
| **MÃºltiplas transaÃ§Ãµes no mesmo comerciante em menos de 10 segundos** | ğŸš¨ **Alerta de possÃ­vel fraude** |

## ğŸ”¹ Estrutura do Projeto
```
aws-glue-pyspark-etl/
â”œâ”€â”€ data/                          # Dados brutos e processados (nÃ£o versionados)
â”œâ”€â”€ notebooks/                     # Jupyter Notebooks com SQL + Python
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb  # AnÃ¡lise exploratÃ³ria inicial
â”‚   â”œâ”€â”€ 02_etl_processing.ipynb    # Pipeline de ETL no PySpark
â”‚   â”œâ”€â”€ 03_sql_optimization.ipynb  # ComparaÃ§Ã£o de SQL puro x PySpark
â”‚   â”œâ”€â”€ 04_power_bi_integration.ipynb  # TransformaÃ§Ã£o de dados para Power BI
â”œâ”€â”€ scripts/                       # CÃ³digo fonte em Python
â”œâ”€â”€ sql/                           # Consultas SQL otimizadas
â”œâ”€â”€ power_bi/                      # Dashboard Power BI
â”œâ”€â”€ docs/                          # DocumentaÃ§Ã£o adicional
â”œâ”€â”€ config/                        # Arquivos de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ schema.json                # EspecificaÃ§Ã£o do schema para validaÃ§Ã£o de dados
â””â”€â”€ README.md                      # DocumentaÃ§Ã£o principal do projeto
```

## ğŸ“Š Resultados e AnÃ¡lises
- **EstatÃ­sticas descritivas antes e depois da transformaÃ§Ã£o.**
- **OtimizaÃ§Ã£o de consultas SQL e comparaÃ§Ã£o de performance.**
- **Dashboards interativos para anÃ¡lise de fraudes.**
- **ValidaÃ§Ã£o da qualidade dos dados pÃ³s-ETL.**

## ğŸ“” Notebooks Criados
1. **01_data_exploration.ipynb** â†’ AnÃ¡lise exploratÃ³ria e estatÃ­sticas iniciais.
2. **02_etl_processing.ipynb** â†’ ImplementaÃ§Ã£o do pipeline ETL no PySpark.
3. **03_sql_optimization.ipynb** â†’ ComparaÃ§Ã£o entre SQL puro e PySpark, otimizando queries.
4. **04_power_bi_integration.ipynb** â†’ PreparaÃ§Ã£o de dados para visualizaÃ§Ã£o no Power BI.

## ğŸ”„ PrÃ³ximos Passos
- Implementar **estrutura de particionamento** para otimizar queries.
- Criar dashboards com **Power BI** para visualizaÃ§Ã£o de padrÃµes de fraude.
- Adicionar **testes automatizados** de qualidade dos dados.
- Integrar com **AWS Lambda** para execuÃ§Ã£o automatizada do ETL.

ğŸš€ Desenvolvido por [Viviana](https://github.com/vivinfor)

