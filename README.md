# AWS Glue & PySpark ETL

## ğŸ“Œ Objetivo
Este projeto demonstra um pipeline de **ETL escalÃ¡vel** usando **AWS Glue e PySpark** para processamento e transformaÃ§Ã£o de dados. Ele inclui tÃ©cnicas para **limpeza, tratamento de outliers, estatÃ­sticas descritivas e armazenamento otimizado no S3**.

## ğŸ”¹ Fonte dos Dados
Os dados utilizados neste projeto sÃ£o provenientes de um dataset pÃºblico disponibilizado pelo Kaggle, referente a **transaÃ§Ãµes financeiras sintÃ©ticas**. O dataset foi gerado atravÃ©s da ferramenta **Sparkov Data Generation**, simulando transaÃ§Ãµes de janeiro a dezembro de 2023. 

### ğŸ“Œ Link para o dataset:
[Kaggle - Fraude em TransaÃ§Ãµes de CartÃ£o de CrÃ©dito](https://www.kaggle.com/competitions/fraude-em-transaes-de-carto-de-crdito/data)

O dataset contÃ©m informaÃ§Ãµes como:
- **Dados da transaÃ§Ã£o** (data, valor, comerciante, categoria, localizaÃ§Ã£o, etc.).
- **InformaÃ§Ãµes do titular do cartÃ£o** (nome, endereÃ§o, gÃªnero, profissÃ£o, etc.).
- **Indicador de fraude (`is_fraud`)** para identificar transaÃ§Ãµes fraudulentas.

## ğŸ”¹ Tecnologias Utilizadas
- **AWS Glue**: Para processamento e transformaÃ§Ã£o de dados em grande escala.
- **PySpark**: Para manipulaÃ§Ã£o e transformaÃ§Ã£o eficiente dos dados.
- **AWS S3**: Para armazenamento dos dados brutos e processados.
- **SQL**: Para anÃ¡lise e validaÃ§Ã£o de qualidade dos dados.

## ğŸš€ Pipeline de ETL
1. **ExtraÃ§Ã£o**: Carregamento do dataset pÃºblico armazenado no **S3**.
2. **TransformaÃ§Ã£o**:
   - RemoÃ§Ã£o de **valores nulos e duplicados**.
   - **ValidaÃ§Ã£o da estrutura do dataset** de acordo com um schema predefinido (`schema.json`).
   - **DetecÃ§Ã£o de outliers** usando **Z-score**.
   - CÃ¡lculo de **estatÃ­sticas descritivas** (mÃ©dia, mediana, desvio padrÃ£o, distribuiÃ§Ã£o de valores faltantes).
   - NormalizaÃ§Ã£o de colunas.
3. **Carga**: Salvamento do dataset transformado no S3, pronto para anÃ¡lise posterior.

## ğŸ”¹ ValidaÃ§Ã£o dos Dados
Durante o pipeline ETL, aplicamos uma sÃ©rie de validaÃ§Ãµes para garantir a qualidade e consistÃªncia dos dados:
- **Estrutura e tipos de dados**: ValidaÃ§Ã£o contra um schema predefinido (`schema.json`).
- **PresenÃ§a de valores nulos**: ExclusÃ£o de registros com campos crÃ­ticos vazios.
- **DetecÃ§Ã£o de outliers**: IdentificaÃ§Ã£o e tratamento de valores atÃ­picos usando **Z-score**.
- **DistribuiÃ§Ã£o dos dados**: VerificaÃ§Ã£o de anomalias estatÃ­sticas.
- **Formato e integridade das datas**: ConversÃ£o de formatos e anÃ¡lise de consistÃªncia temporal.

## ğŸ› ï¸ Como Rodar o Projeto
### ğŸ“Œ PrÃ©-requisitos
- Conta AWS com permissÃµes para Glue e S3.
- Python 3.8+
- PySpark instalado (`pip install pyspark`)

### ğŸ“Œ ConfiguraÃ§Ã£o do Ambiente
1. **Clone o repositÃ³rio:**
```sh
 git clone https://github.com/vivinfor/aws-glue-pyspark-etl.git
 cd aws-glue-pyspark-etl
```
2. **Crie um ambiente virtual e instale as dependÃªncias:**
```sh
 python -m venv venv
 source venv/bin/activate  # Linux/Mac
 venv\Scripts\activate  # Windows
 pip install -r requirements.txt
```
3. **Defina as variÃ¡veis de ambiente** no `.env`:
```sh
 AWS_ACCESS_KEY=your-access-key
 AWS_SECRET_KEY=your-secret-key
 S3_BUCKET_NAME=your-bucket
```
4. **Execute o script PySpark localmente:**
```sh
 python etl.py
```

## ğŸ“Š Resultados e AnÃ¡lises
- **EstatÃ­sticas descritivas antes e depois da transformaÃ§Ã£o.**
- **DetecÃ§Ã£o e tratamento de outliers.**
- **ValidaÃ§Ã£o da qualidade dos dados pÃ³s-ETL.**

## ğŸ”„ PrÃ³ximos Passos
- Integrar com **AWS Lambda** para execuÃ§Ã£o automatizada do ETL.
- Criar uma **estrutura de particionamento** para otimizar queries.
- Implementar **testes automatizados** de qualidade dos dados.
- Criar dashboards com **Plotly** ou **Power BI** para anÃ¡lise de fraudes.

ğŸš€ **Desenvolvido por [Viviana]**
