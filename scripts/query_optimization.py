import os
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, max, min

# Criar sess√£o Spark
spark = SparkSession.builder \
    .appName("Query Optimization - PySpark vs SQL") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# Caminho do diret√≥rio processado
data_path = "data/processed"

# Verificar se os dados processados existem
if not os.path.exists(data_path):
    raise FileNotFoundError(f"‚ùå Diret√≥rio '{data_path}' n√£o encontrado! Execute o ETL primeiro.")

# Carregar dados processados
print("üìÇ Carregando dados processados...")
df = spark.read.parquet(data_path)
print(f"‚úÖ Total de registros carregados: {df.count()}")

# Criar View Tempor√°ria para SQL Queries
df.createOrReplaceTempView("transactions")

# Fun√ß√£o para medir tempo de execu√ß√£o
def execute_query(query, description):
    start_time = time.time()
    result = spark.sql(query)
    result.show(5)
    elapsed_time = time.time() - start_time
    print(f"‚è≥ Tempo para {description}: {elapsed_time:.4f} segundos")
    return result

# üîπ 1Ô∏è‚É£ Consulta B√°sica: Contagem de Transa√ß√µes
execute_query("""
    SELECT COUNT(*) as total_transacoes
    FROM transactions
""", "contar total de transa√ß√µes")

# üîπ 2Ô∏è‚É£ Agrega√ß√£o por Categoria
execute_query("""
    SELECT category, COUNT(*) as num_transacoes, AVG(amt) as avg_value
    FROM transactions
    GROUP BY category
    ORDER BY num_transacoes DESC
""", "agrega√ß√£o por categoria")

# üîπ 3Ô∏è‚É£ Transa√ß√µes com Maior Valor por Categoria
execute_query("""
    SELECT category, MAX(amt) as max_value
    FROM transactions
    GROUP BY category
    ORDER BY max_value DESC
""", "transa√ß√µes de maior valor")

# üîπ 4Ô∏è‚É£ Impacto do Particionamento
partitioned_df = df.repartition("category")
partitioned_df.createOrReplaceTempView("transactions_partitioned")
execute_query("""
    SELECT category, COUNT(*) as num_transacoes
    FROM transactions_partitioned
    GROUP BY category
""", "contagem por categoria (dados particionados)")

print("‚úÖ Testes de performance conclu√≠dos!")

# Finalizar sess√£o Spark
spark.stop()
