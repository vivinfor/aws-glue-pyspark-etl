import os
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, max, min

# ğŸ“Œ Criar sessÃ£o Spark
spark = SparkSession.builder \
    .appName("Query Optimization - PySpark vs SQL") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# ğŸ“‚ Caminho do diretÃ³rio processado
data_path = "data/processed"

# ğŸ“Œ Verificar se os dados processados existem
if not os.path.exists(data_path):
    raise FileNotFoundError(f"âŒ DiretÃ³rio '{data_path}' nÃ£o encontrado! Execute o ETL primeiro.")

# ğŸ“‚ Carregar dados processados
print("ğŸ“‚ Carregando dados processados...")
df = spark.read.parquet(data_path)
print(f"âœ… Total de registros carregados: {df.count()}")

# ğŸ“Œ Criar View TemporÃ¡ria para SQL Queries
df.createOrReplaceTempView("transactions")

# ğŸ“Œ Garantir que as colunas esperadas estÃ£o disponÃ­veis
expected_columns = {
    "trans_date_trans_time", "cc_num", "merchant", "category", "amt",
    "first", "last", "gender", "street", "city", "state", "zip",
    "lat", "long", "city_pop", "job", "dob", "trans_num", "unix_time",
    "merch_lat", "merch_long", "is_fraud", "day_of_week", "hour_of_day",
    "transaction_period", "possible_fraud_high_value", "possible_fraud_fast_transactions"
}
actual_columns = set(df.columns)

# ğŸ”¥ Verificar colunas inesperadas
extra_columns = actual_columns - expected_columns
if extra_columns:
    print(f"âš ï¸ Removendo colunas extras: {extra_columns}")
    df = df.drop(*extra_columns)

# ğŸ”¥ Verificar colunas faltantes
missing_columns = expected_columns - actual_columns
if missing_columns:
    raise ValueError(f"âŒ Colunas faltantes no dataset processado: {missing_columns}")

# ğŸ“Œ Criar View TemporÃ¡ria novamente apÃ³s ajuste
df.createOrReplaceTempView("transactions")

# ğŸ“Œ FunÃ§Ã£o para medir tempo de execuÃ§Ã£o de queries
def execute_query(query, description):
    start_time = time.time()
    result = spark.sql(query)
    result.show(5)
    elapsed_time = time.time() - start_time
    print(f"â³ Tempo para {description}: {elapsed_time:.4f} segundos")
    return result

# ğŸ”¹ 1ï¸âƒ£ Consulta BÃ¡sica: Contagem de TransaÃ§Ãµes
execute_query("""
    SELECT COUNT(*) as total_transacoes
    FROM transactions
""", "contar total de transaÃ§Ãµes")

# ğŸ”¹ 2ï¸âƒ£ AgregaÃ§Ã£o por Categoria
execute_query("""
    SELECT category, COUNT(*) as num_transacoes, AVG(amt) as avg_value
    FROM transactions
    GROUP BY category
    ORDER BY num_transacoes DESC
""", "agregaÃ§Ã£o por categoria")

# ğŸ”¹ 3ï¸âƒ£ TransaÃ§Ãµes com Maior Valor por Categoria
execute_query("""
    SELECT category, MAX(amt) as max_value
    FROM transactions
    GROUP BY category
    ORDER BY max_value DESC
""", "transaÃ§Ãµes de maior valor")

# ğŸ”¹ 4ï¸âƒ£ Contagem de fraudes por perÃ­odo do dia
execute_query("""
    SELECT transaction_period, COUNT(*) as num_fraudes
    FROM transactions
    WHERE is_fraud = 1
    GROUP BY transaction_period
    ORDER BY num_fraudes DESC
""", "contagem de fraudes por perÃ­odo do dia")

# ğŸ”¹ 5ï¸âƒ£ Impacto do Particionamento
partitioned_df = df.repartition("category")
partitioned_df.createOrReplaceTempView("transactions_partitioned")
execute_query("""
    SELECT category, COUNT(*) as num_transacoes
    FROM transactions_partitioned
    GROUP BY category
""", "contagem por categoria (dados particionados)")

print("âœ… Testes de performance concluÃ­dos!")

# ğŸ“Œ Finalizar sessÃ£o Spark
spark.stop()
