import os
import json
import yaml
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, count, mean, stddev, unix_timestamp, lag, date_format, lit
)
from pyspark.sql.window import Window

# ğŸ“Œ Configurar logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# ğŸ“‚ ConfiguraÃ§Ã£o dos caminhos
CONFIG_PATH = "config/config.yaml"
SCHEMA_PATH = "config/schema.json"
VALIDATION_RULES_PATH = "config/validation_rules.yaml"

# ğŸ“‚ Verificar se os arquivos de configuraÃ§Ã£o existem
for path in [CONFIG_PATH, SCHEMA_PATH, VALIDATION_RULES_PATH]:
    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ Arquivo '{path}' nÃ£o encontrado!")

# ğŸ“‚ Carregar os arquivos de configuraÃ§Ã£o
with open(CONFIG_PATH, "r") as f:
    config = yaml.safe_load(f)

with open(SCHEMA_PATH, "r") as f:
    schema = json.load(f)

with open(VALIDATION_RULES_PATH, "r") as f:
    validation_rules = yaml.safe_load(f)

# ğŸ“‚ Determinar caminho de dados processados
DATA_PATH = os.path.abspath(config["data_path"])
logger.info(f"ğŸ“‚ DiretÃ³rio de dados processados: {DATA_PATH}")

# ğŸš€ Criar sessÃ£o Spark
spark = SparkSession.builder.appName("ValidaÃ§Ã£o de Dados - ETL").getOrCreate()

# ğŸ“‚ Carregar os dados processados
df = spark.read.parquet(DATA_PATH)

# âœ… **1. Verificar colunas esperadas**
expected_columns = {field["name"] for field in schema["fields"]}
actual_columns = set(df.columns)
missing_columns = expected_columns - actual_columns

if missing_columns:
    logger.warning(f"âš ï¸ Colunas ausentes no dataset: {missing_columns}")
else:
    logger.info("âœ… Todas as colunas esperadas estÃ£o presentes.")

# âœ… **2. Validar tipos de dados**
type_mapping = {
    "string": "string",
    "int": "integer",
    "double": "double",
    "timestamp": "timestamp"
}

type_issues = []
for field in schema["fields"]:
    col_name = field["name"]
    expected_type = type_mapping.get(field["type"], "unknown")
    
    if col_name in df.columns:
        actual_type = df.select(col_name).schema[0].dataType.simpleString()
        if expected_type == "double":
            expected_type = "float"
        if actual_type != expected_type:
            type_issues.append(f"âš ï¸ Tipo incorreto em `{col_name}`: Esperado {expected_type}, encontrado {actual_type}")

if type_issues:
    for issue in type_issues:
        logger.warning(issue)
else:
    logger.info("âœ… Todos os tipos de dados estÃ£o corretos.")

# âœ… **3. Verificar valores nulos crÃ­ticos**
for col_name in validation_rules["validation"]["missing_values"]["critical"]:
    null_count = df.filter(col(col_name).isNull()).count()
    if null_count > 0:
        logger.error(f"âŒ {null_count} registros possuem `{col_name}` nulo e devem ser removidos!")

df = df.dropna(subset=validation_rules["validation"]["missing_values"]["critical"])

# âœ… **4. Tratar valores nulos nÃ£o crÃ­ticos**
fill_values = validation_rules["validation"]["missing_values"]["non_critical"]
df = df.fillna(fill_values)
logger.info("âœ… Valores nulos nÃ£o crÃ­ticos preenchidos conforme regras definidas.")

# âœ… **5. Detectar outliers em `amt`**
if validation_rules["validation"]["outlier_detection"]["amt"]["method"] == "zscore":
    threshold = validation_rules["validation"]["outlier_detection"]["amt"]["threshold"]
    amt_stats = df.select(mean("amt").alias("mean_amt"), stddev("amt").alias("std_amt")).collect()[0]
    mean_amt, std_amt = amt_stats["mean_amt"], amt_stats["std_amt"]
    outliers = df.filter((col("amt") > mean_amt + threshold * std_amt) | (col("amt") < mean_amt - threshold * std_amt))
    if outliers.count() > 0:
        logger.warning(f"âš ï¸ {outliers.count()} registros detectados como outliers em `amt`. Sugerido remover!")

# âœ… **6. ValidaÃ§Ã£o de fraudes**
if validation_rules["validation"]["fraud_detection"]["multi_state_check"]:
    df = df.withColumn("date", col("trans_date_trans_time").cast("date"))
    multi_state_purchases = df.groupBy("cc_num", "date").agg(count("state").alias("state_count")).filter(col("state_count") > 1)
    if multi_state_purchases.count() > 0:
        logger.warning(f"âš ï¸ {multi_state_purchases.count()} cartÃµes usados em mÃºltiplos estados no mesmo dia!")

# âœ… **7. Verificar duplicatas**
if validation_rules["validation"]["duplicates"]["check"]:
    duplicate_count = df.count() - df.dropDuplicates().count()
    if duplicate_count > 0:
        logger.warning(f"âš ï¸ {duplicate_count} registros duplicados encontrados!")
    else:
        logger.info("âœ… Nenhuma duplicata encontrada.")

logger.info("âœ… ValidaÃ§Ã£o de dados concluÃ­da com sucesso!")

# ğŸš€ **Encerrar sessÃ£o Spark**
spark.stop()
