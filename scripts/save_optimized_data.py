import os
import subprocess
import yaml
import logging
import shutil
import sys

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, mean, stddev, lit
from pyspark.sql.types import DoubleType, IntegerType

# ğŸ“Œ ConfiguraÃ§Ã£o de logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# ğŸ“‚ Carregar ConfiguraÃ§Ã£o do YAML
config_path = os.path.abspath("config/config.yaml")
if not os.path.exists(config_path):
    raise FileNotFoundError("âŒ Arquivo 'config.yaml' nÃ£o encontrado!")

with open(config_path, "r") as f:
    config = yaml.safe_load(f)

PROCESSED_DATA_DIR = os.path.abspath(config["data_path"])
OPTIMIZED_DATA_DIR = os.path.abspath(config["optimized_data_path"])

logger.info(f"ğŸ“‚ DiretÃ³rio de dados processados: {PROCESSED_DATA_DIR}")
logger.info(f"ğŸ“‚ DiretÃ³rio de dados otimizados: {OPTIMIZED_DATA_DIR}")

# âœ… **Executar `data_validation.py` antes de otimizar os dados**
logger.info("ğŸ” Iniciando validaÃ§Ã£o de dados antes da otimizaÃ§Ã£o...")
validation_script = "scripts/data_validation.py"

try:
    result = subprocess.run([sys.executable, validation_script], check=True, text=True, capture_output=True)
    logger.info(f"ğŸ“Š Resultado da validaÃ§Ã£o:\n{result.stdout}")
except subprocess.CalledProcessError as e:
    logger.error(f"ğŸš¨ Erro ao validar os dados! SaÃ­da:\n{e.stderr}")
    exit(1)

# ğŸš€ Criar sessÃ£o Spark
spark = SparkSession.builder.appName("Save Optimized Data").getOrCreate()

# ğŸ“Œ Carregar os dados processados
logger.info("ğŸ“‚ Carregando dados processados apÃ³s validaÃ§Ã£o...")
df = spark.read.parquet(PROCESSED_DATA_DIR)

# ğŸ”¥ Remover colunas desnecessÃ¡rias
columns_to_remove = {"profile", "acct_num", "time_diff"}
df = df.drop(*columns_to_remove)
logger.info(f"âœ… Removidas colunas desnecessÃ¡rias: {columns_to_remove}")

# ğŸ”¥ **Corrigir tipos incorretos**
df = df.withColumn("zip", col("zip").cast(IntegerType()))
df = df.withColumn("lat", col("lat").cast(DoubleType()))
df = df.withColumn("long", col("long").cast(DoubleType()))
df = df.withColumn("city_pop", col("city_pop").cast(IntegerType()))
df = df.withColumn("unix_time", col("unix_time").cast(IntegerType()))
df = df.withColumn("amt", col("amt").cast(DoubleType()))
df = df.withColumn("is_fraud", col("is_fraud").cast(IntegerType()))
df = df.withColumn("merch_lat", col("merch_lat").cast(DoubleType()))
df = df.withColumn("merch_long", col("merch_long").cast(DoubleType()))

logger.info("âœ… CorreÃ§Ã£o de tipos aplicada.")

# ğŸ“Œ **Garantir que valores nulos tenham tratamento adequado**
df = df.fillna({
    "merchant": "Desconhecido",
    "merch_lat": 0.0,
    "merch_long": 0.0
})
logger.info("âœ… Valores nulos tratados.")

# ğŸ” **ValidaÃ§Ãµes antes de salvar**
total_registros = df.count()
total_transacoes_fraude = df.filter(col("is_fraud") == 1).count()
media_amt = df.select(mean("amt")).collect()[0][0]

logger.info(f"ğŸ“Š Total de registros: {total_registros}")
logger.info(f"ğŸ“Š Total de transaÃ§Ãµes fraudulentas: {total_transacoes_fraude}")
logger.info(f"ğŸ“Š MÃ©dia de valores de transaÃ§Ã£o (amt): {media_amt:.2f}")

# ğŸ“Œ **Garantir que o diretÃ³rio otimizado estÃ¡ pronto para salvar os dados**
if os.path.exists(OPTIMIZED_DATA_DIR):
    logger.info(f"ğŸ—‘ï¸ DiretÃ³rio existente detectado: {OPTIMIZED_DATA_DIR}. Removendo para recriar...")
    shutil.rmtree(OPTIMIZED_DATA_DIR)

os.makedirs(OPTIMIZED_DATA_DIR)
logger.info(f"ğŸ“‚ DiretÃ³rio recriado: {OPTIMIZED_DATA_DIR}")

# ğŸ“Œ **Salvar os dados otimizados em formato Parquet com particionamento por categoria**
df.write.mode("overwrite").partitionBy("category").parquet(OPTIMIZED_DATA_DIR)
logger.info("âœ… Dados otimizados salvos com sucesso!")

# ğŸ“‚ **Testar leitura do dataset salvo**
logger.info("ğŸ“‚ Testando leitura dos dados otimizados...")
df_test = spark.read.parquet(OPTIMIZED_DATA_DIR)
logger.info(f"âœ… Registros lidos: {df_test.count()}")
logger.info("ğŸ¯ Estrutura do dataset otimizado:")
df_test.printSchema()

logger.info("ğŸš€ Processo de otimizaÃ§Ã£o concluÃ­do!")
spark.stop()
