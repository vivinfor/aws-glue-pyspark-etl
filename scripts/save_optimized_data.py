import os
import yaml
import shutil
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import DoubleType, StringType, IntegerType

# ğŸ“‚ Carregar ConfiguraÃ§Ã£o do YAML
config_path = os.path.abspath("config/config.yaml")
print(f"ğŸ“‚ Tentando carregar: {config_path}")

if os.path.exists(config_path):
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    print("âœ… ConfiguraÃ§Ã£o carregada com sucesso!")
else:
    raise FileNotFoundError("âŒ Arquivo 'config.yaml' nÃ£o encontrado!")

# ğŸ“‚ Definir caminhos usando o config.yaml
PROCESSED_DATA_DIR = os.path.normpath(config.get("data_path", "data/processed/"))
OPTIMIZED_DATA_DIR = os.path.normpath(config.get("optimized_data_path", "data/optimized/"))

if not os.path.isabs(PROCESSED_DATA_DIR):
    PROCESSED_DATA_DIR = os.path.abspath(PROCESSED_DATA_DIR)
if not os.path.isabs(OPTIMIZED_DATA_DIR):
    OPTIMIZED_DATA_DIR = os.path.abspath(OPTIMIZED_DATA_DIR)

print(f"ğŸ“‚ DiretÃ³rio de dados processados: {PROCESSED_DATA_DIR}")
print(f"ğŸ“‚ DiretÃ³rio de dados otimizados: {OPTIMIZED_DATA_DIR}")

# ğŸš€ Criar sessÃ£o Spark
spark = SparkSession.builder.appName("Save Optimized Data").getOrCreate()

# ğŸ“Œ Ler os dados processados
print("ğŸ“‚ Carregando dados processados...")
df = spark.read.parquet(PROCESSED_DATA_DIR)
print(f"âœ… Total de registros carregados: {df.count()}")

# ğŸ“Œ Colunas esperadas conforme o schema atualizado
expected_columns = {
    "ssn", "trans_date_trans_time", "cc_num", "merchant", "category", "amt",
    "first", "last", "gender", "street", "city", "state", "zip", "lat", "long",
    "city_pop", "job", "dob", "trans_num", "unix_time", "merch_lat", "merch_long",
    "is_fraud", "day_of_week", "hour_of_day", "transaction_period",
    "possible_fraud_high_value", "possible_fraud_fast_transactions"
}
actual_columns = set(df.columns)

# ğŸ”¥ Verificar colunas inesperadas e removÃª-las
extra_columns = actual_columns - expected_columns
if extra_columns:
    print(f"âš ï¸ Removendo colunas inesperadas: {extra_columns}")
    df = df.drop(*extra_columns)

# ğŸ”¥ Verificar colunas faltantes
missing_columns = expected_columns - actual_columns
if missing_columns:
    raise ValueError(f"âŒ Colunas faltantes no dataset processado: {missing_columns}")

# âœ… **CorreÃ§Ã£o: Garantir tipos corretos**
df = df.withColumn("amt", col("amt").cast(DoubleType()))
df = df.withColumn("day_of_week", col("day_of_week").cast(StringType()))
df = df.withColumn("hour_of_day", col("hour_of_day").cast(IntegerType()))
df = df.withColumn("possible_fraud_high_value", col("possible_fraud_high_value").cast(IntegerType()))
df = df.withColumn("possible_fraud_fast_transactions", col("possible_fraud_fast_transactions").cast(IntegerType()))
df = df.withColumn("category", col("category").cast(StringType()))
print("ğŸ”„ ConversÃ£o aplicada aos campos de tipo!")

# ğŸ“Œ Garantir que o diretÃ³rio otimizado estÃ¡ pronto para salvar os dados
if os.path.exists(OPTIMIZED_DATA_DIR):
    print(f"ğŸ—‘ï¸ DiretÃ³rio existente detectado: {OPTIMIZED_DATA_DIR}. Removendo para recriar...")
    shutil.rmtree(OPTIMIZED_DATA_DIR)

os.makedirs(OPTIMIZED_DATA_DIR)
print(f"ğŸ“‚ DiretÃ³rio recriado: {OPTIMIZED_DATA_DIR}")

# ğŸ“Œ Salvar os dados otimizados em formato Parquet com particionamento por categoria
df.write.mode("overwrite").partitionBy("category").parquet(OPTIMIZED_DATA_DIR)
print("âœ… Dados otimizados salvos com sucesso!")

# ğŸ“‚ Testar leitura do dataset salvo
print("ğŸ“‚ Testando leitura dos dados otimizados...")
df_test = spark.read.parquet(OPTIMIZED_DATA_DIR)
print(f"âœ… Registros lidos: {df_test.count()}")
print("ğŸ¯ Estrutura do dataset otimizado:")
df_test.printSchema()

print("ğŸš€ Processo de otimizaÃ§Ã£o concluÃ­do!")
spark.stop()
